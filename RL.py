# -*- coding: utf-8 -*-
"""RL.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AoJ97tAroI6LkK_frQnnrOtgRoEeD6H8
"""

import numpy as np
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from collections import deque
from scipy.integrate import odeint
import matplotlib.pyplot as plt

# ==== Parameters ====
Cin_max = 0.1      # g/L (nutrient)
q_total = 0.5       # L/h
gamma = 4.4e11
umax = 1.0
Km = 6.85e-5
N0 = 1e10
TARGET = 3e10
C_D_TARGET = 10.0  # Target product D concentration (adjust as needed)

sampling_time = 0.3

TOTAL_VOLUME = 0.02   # L (20 mL reactor)
TOTAL_FLOW = q_total * TOTAL_VOLUME    # L/h (or your actual system input)


# ==== Environment ====
class SingleSubstrateChemostatEnv:
    def __init__(self):
        self.reset()





    def monod(self, C):
        return umax * C / (Km + C)



    def reaction_step(self, A_ratio, B_ratio, C_ratio):
        input_CA, input_CB, input_CC = 100, 200, 370  # g/L
        input_flow = (1 - self.Cin_frac) * self.q * TOTAL_VOLUME  # L/h
        dt = sampling_time

        # Feed (g/L per timestep)
        flow_A = (input_flow / TOTAL_VOLUME) * A_ratio * input_CA * dt
        flow_B = (input_flow / TOTAL_VOLUME) * B_ratio * input_CB * dt
        flow_C = (input_flow / TOTAL_VOLUME) * C_ratio * input_CC * dt

        self.C_A += flow_A
        self.C_B += flow_B
        self.C_C += flow_C

        # Reaction
        r1 = 0.8 * min(self.C_A, self.C_B / 2, self.C_C / 3)
        self.C_D += 2 * r1
        self.C_A -= r1
        self.C_B -= 2 * r1
        self.C_C -= 3 * r1

        # Dilution
        dilution = self.q * dt
        self.C_A *= (1 - dilution)
        self.C_B *= (1 - dilution)
        self.C_C *= (1 - dilution)
        self.C_D *= (1 - dilution)



    def model(self, vars, t):
        N, C = vars
        mu = self.monod(C)
        dN = (mu - self.q) * N
        dC = self.q * (self.Cin - C) - (1 / gamma) * mu * N
        return [dN, dC]

    def reset(self):
        self.N = N0
        self.C = 0.0
        self.C_A = 0.0
        self.C_B = 0.0
        self.C_C = 0.0
        self.C_D = 0.0

        self.history = {
            'N': [], 'C_frac': [], 'Cin_frac': [],
            'ratios': [], 'C_D': []
        }
        return np.array([self.N / TARGET, self.C_D / C_D_TARGET], dtype=np.float32)



    def step(self, action):
        action = np.clip(action, 0, 1.0)

        self.Cin_frac = float(action[0])
        self.q = q_total
        self.Cin = self.Cin_frac * Cin_max

        # ratios_raw = np.array(action[1:4])
        # ratios = ratios_raw / (ratios_raw.sum() + 1e-8)
        # A_ratio, B_ratio, C_ratio = ratios
        A_ratio, B_ratio, C_ratio = action[1:4]

        self.reaction_step(A_ratio, B_ratio, C_ratio)

        vars0 = [self.N, self.C]
        sol = odeint(self.model, vars0, [0, sampling_time])
        self.N, self.C = sol[-1]

        self.history['N'].append(self.N)
        self.history['C_frac'].append(self.Cin_frac)
        self.history['Cin_frac'].append(self.Cin_frac)
        self.history['ratios'].append([A_ratio, B_ratio, C_ratio])
        self.history['C_D'].append(self.C_D)

        # Reward combines biomass + product)

        done = False
        reward_pop = -abs(self.N - TARGET) / TARGET
        # reward_prod = np.exp(-((self.C_D - 35) ** 2) / (2 * 5**2))  # 5 is stddev
        reward_prod = -abs(self.C_D - C_D_TARGET) / C_D_TARGET

        reward = 0.7 * reward_pop + 0.3* reward_prod
        return np.array([self.N / TARGET, self.C_D / C_D_TARGET], dtype=np.float32), reward, done, reward_pop, reward_prod








# ==== RL Agent ====
class Actor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.out = nn.Linear(64, action_dim)
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        out = self.out(x)  # shape: (batch_size, 4)

        cin_frac = 0.75 + 0.25 * torch.tanh(out[:, 0:1])  # [0.5, 1]
        abc_raw = F.softmax(out[:, 1:4], dim=1)           # sum=1, [0,1]

        return torch.cat([cin_frac, abc_raw], dim=1)



class Critic(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim + action_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.out = nn.Linear(64, 1)

    def forward(self, state, action):
        x = torch.cat([state, action], dim=1)
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        return self.out(x)

class DDPGAgent:
    def __init__(self, state_dim, action_dim):
        self.actor = Actor(state_dim, action_dim)
        self.target_actor = Actor(state_dim, action_dim)
        self.critic = Critic(state_dim, action_dim)
        self.target_critic = Critic(state_dim, action_dim)
        self.memory = deque(maxlen=2000)
        self.gamma = 0.99
        self.tau = 0.005
        self.actor_optim = optim.Adam(self.actor.parameters(), lr=1e-3)
        self.critic_optim = optim.Adam(self.critic.parameters(), lr=2e-3)
        self.noise_std = 0.1         # Initial noise
        self.noise_decay = 0.98      # Decay rate per episode
        self.min_noise = 0.005       # Min noise limit


    def decay_noise(self):
        self.noise_std = max(self.min_noise, self.noise_std * self.noise_decay)

    def act(self, state):
        state = torch.FloatTensor(state).unsqueeze(0)
        action = self.actor(state).detach().numpy().flatten()
        noise = np.random.normal(0, 0.01, size=1)
        return np.clip(action + noise, 0, 1.0)

    def remember(self, s, a, r, s2):
        self.memory.append((s, a, r, s2))

    def train(self, batch_size=32):
        if len(self.memory) < batch_size:
            return
        batch = random.sample(self.memory, batch_size)
        for s, a, r, s2 in batch:
            s = torch.FloatTensor(s).unsqueeze(0)
            s2 = torch.FloatTensor(s2).unsqueeze(0)
            a = torch.FloatTensor(a).unsqueeze(0)
            r = torch.tensor([r], dtype=torch.float32)

            a2 = self.target_actor(s2)
            target_q = r + self.gamma * self.target_critic(s2, a2).detach()
            q_val = self.critic(s, a)

            critic_loss = F.mse_loss(q_val, target_q)
            self.critic_optim.zero_grad()
            critic_loss.backward()
            self.critic_optim.step()

            actor_loss = -self.critic(s, self.actor(s)).mean()
            self.actor_optim.zero_grad()
            actor_loss.backward()
            self.actor_optim.step()

            for t, p in zip(self.target_actor.parameters(), self.actor.parameters()):
                t.data.copy_(self.tau * p.data + (1 - self.tau) * t.data)
            for t, p in zip(self.target_critic.parameters(), self.critic.parameters()):
                t.data.copy_(self.tau * p.data + (1 - self.tau) * t.data)
# ==== Training ====
env = SingleSubstrateChemostatEnv()
agent = DDPGAgent(state_dim=2, action_dim=4)


all_history = []

for episode in range(24):
    state = env.reset()
    episode_rewards = []
    reward_pop_list = []
    reward_prod_list = []
    # agent.decay_noise()
    for step in range(200):
        action = agent.act(state)
        next_state, reward, done, r_pop, r_prod = env.step(action)

        episode_rewards.append(reward)
        reward_pop_list.append(r_pop)
        reward_prod_list.append(r_prod)

        agent.remember(state, action, reward, next_state)
        agent.train()
        state = next_state
        if step % 50 == 0:
            print(f"Cin_frac: {action[0]:.3f} | A: {action[1]:.3f}, B: {action[2]:.3f}, C: {action[3]:.3f}")

    total_reward = sum(episode_rewards)
    print(f"Episode {episode+1} | Total Reward: {total_reward:.3f} | Pop: {sum(reward_pop_list):.3f} | Prod: {sum(reward_prod_list):.3f}")

    # Deep copy of history
    all_history.append({
        'N': env.history['N'][:],
        'C_frac': env.history['C_frac'][:],
        'Cin_frac': env.history['Cin_frac'][:],
        'C_D': env.history['C_D'][:],
        'ratios': env.history['ratios'][:]
    })

# ==== Plotting Each Episode Separately ====
for i, h in enumerate(all_history):
    ratios = np.array(h['ratios'])
    cin_array = np.array(h['Cin_frac'])

    plt.figure(figsize=(12, 8))
    plt.subplot(4, 1, 1)
    plt.plot(h['N'], label='Cell Count (N)')
    plt.axhline(TARGET, color='r', linestyle='--', label='Target')
    plt.ylabel("cells/L")
    plt.legend()
    plt.grid()

    plt.subplot(4, 1, 2)
    plt.plot(166.67 * cin_array, label='Cin Fraction', color='green')
    plt.ylabel("Cin/Cin_max")
    plt.legend()
    plt.grid()

    plt.subplot(4, 1, 3)
    plt.plot(h['C_D'], label='Product D', color='blue')
    plt.ylabel("Concentration")
    plt.legend()
    plt.grid()

    plt.subplot(4, 1, 4)
    plt.plot(ratios[:, 0] * 166.67 * (1 - cin_array), label='A_ratio')
    plt.plot(ratios[:, 1] * 166.67 * (1 - cin_array), label='B_ratio')
    plt.plot(ratios[:, 2] * 166.67 * (1 - cin_array), label='C_ratio')
    plt.xlabel("Time Step")
    plt.ylabel("Ratios")
    plt.legend()
    plt.grid()

    plt.tight_layout()
    plt.show()




# # ==== Q-profile Plot ====
# plt.figure(figsize=(10, 6))
# for step, actions, q_vals in q_profiles:
# plt.plot(actions, q_vals, label=f"Step {step}")
# plt.xlabel("Action (Cin fraction)")
# plt.ylabel("Q-value")
# plt.title("Critic's Q-value for Different Actions (Episode 0)")
# plt.legend()
# plt.grid()
# plt.tight_layout()
# plt.show()